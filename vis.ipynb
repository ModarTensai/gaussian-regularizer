{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DARK = False\n",
    "import matplotlib.pyplot as plt\n",
    "if DARK:\n",
    "    plt.style.use('dark_background')\n",
    "import torch\n",
    "from main import Trainer\n",
    "from itertools import product\n",
    "from argparse import Namespace as ns\n",
    "from vis import results, print_results\n",
    "\n",
    "## Explore the results\n",
    "# probe the results by:\n",
    "# - providing a single value or a list of options for each argument\n",
    "# - a function that filters out the arguments you want\n",
    "# out = results(model='lenet', dataset='cifar10', sigma=lambda x: x <= 0.5, aug=[0, 1], exp=0)\n",
    "\n",
    "# print_results(out)  # you can print the results in lines\n",
    "# print_results(out, plot=True)  # or plot them, yay!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tol = {\n",
    "    # these are accuracy tolerance from the baseline accuracy\n",
    "    # increase these tolerance values in case you get this exception:\n",
    "    #   ValueError: max() arg is an empty sequence\n",
    "    ('lenet', 'mnist'): 0,\n",
    "    ('lenet', 'cifar10'): 0.0039,\n",
    "    ('lenet', 'cifar100'): 0.0075, # 0,\n",
    "    ('alexnet', 'cifar10'): 0.0168,\n",
    "    ('alexnet', 'cifar100'): 0.0483,\n",
    "    ('vgg16', 'cifar10'): 0.0236,\n",
    "    ('vgg16', 'cifar100'): 0,\n",
    "}\n",
    "def get(model, dataset, **kwargs):\n",
    "    return [ns(sig=r['sig'], aug=r['aug'], exp=r['exp'],\n",
    "               epochs=r['summary'].last_epoch,\n",
    "               time=sum(r['summary'].time),\n",
    "               acc=r['summary'].test.accuracy,\n",
    "               rob=r['summary'].test.robustness)\n",
    "            for r in results(model=model, dataset=dataset, **kwargs)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance comparison for training with augmentation vs training with the expectation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def results_table(model, dataset, plot=False, fontsize=15, figsize=(8, 5)):\n",
    "    clean = get(model, dataset, aug=0, exp=0)[0]\n",
    "    aug = get(model, dataset, aug=lambda x: x > 0, exp=0)\n",
    "    exp = get(model, dataset, aug=0, exp=lambda x: x > 0)\n",
    "    augs = sorted(set(a.aug for a in aug).union(set(a.aug for a in exp)))\n",
    "    sigs = sorted(set(s.sig for s in aug).union(set(a.sig for a in exp)))\n",
    "\n",
    "    # aug = [a for a in aug if a.aug==10]\n",
    "    # print(clean.time)\n",
    "    # print(sum(a.time for a in aug)/len(aug))\n",
    "    # print(sum(a.time for a in exp)/len(exp))\n",
    "    \n",
    "    def extract(robustness):\n",
    "        out = torch.empty(len(augs), len(sigs)).fill_(float('nan'))\n",
    "        for r in aug:\n",
    "            loc = (augs.index(r.aug), sigs.index(r.sig))\n",
    "            out[loc] = r.rob if robustness else r.acc\n",
    "        for s in sigs:\n",
    "            sub_exp = [r for r in exp if r.sig == s]\n",
    "#             if robustness:\n",
    "#                 print('\\n'.join(str(e) for e in sub_exp) + '\\n'+'#'*5)\n",
    "            acc = [r.acc for r in sub_exp]\n",
    "            loc = acc.index(max(acc))\n",
    "            out[0, sigs.index(s)] = sub_exp[loc].rob if robustness else sub_exp[loc].acc\n",
    "        baseline = clean.rob if robustness else clean.acc\n",
    "        if plot:\n",
    "            mn = min(out[out == out].min().item(), baseline)\n",
    "            mx = max(out[out == out].max().item(), baseline)\n",
    "            ys = torch.linspace(mn, mx, 8)\n",
    "            title = 'Robustness $\\Re$' if robustness else 'Testing Accuracy'\n",
    "            plt.figure(figsize=figsize)\n",
    "            plt.axhline(y=baseline, linewidth=2, color='k', linestyle='--')\n",
    "            plt.plot(out.t().numpy(), '8-')\n",
    "            plt.legend(['Baseline', 'Ours']+[f'aug = {a}' for a in augs[1:]], fontsize=fontsize*.9)\n",
    "            plt.xlabel('Training $\\sigma$', fontsize=fontsize)\n",
    "            plt.ylabel(title, fontsize=fontsize)\n",
    "            plt.yticks([round(float(y), 4) for y in ys], fontsize=fontsize)\n",
    "            plt.xticks(range(len(sigs)), [str(s) for s in sigs], fontsize=fontsize)\n",
    "            plt.grid()\n",
    "            plt.show()\n",
    "        return baseline, out\n",
    "        \n",
    "    acc = extract(False)\n",
    "    rob = extract(True)\n",
    "    \n",
    "    return acc, rob\n",
    "        \n",
    "for model, dataset in tol.keys():\n",
    "    print(model, dataset)\n",
    "    acc, rob = results_table(model, dataset, plot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fair comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comparable_results(model, dataset, plot=False, fontsize=15, figsize=(8, 5), tol=tol.copy()):\n",
    "    clean = get(model, dataset, aug=0, exp=0)[0]\n",
    "    some = lambda res: list(filter(lambda x: x.acc > clean.acc - tol[model, dataset], res))\n",
    "    aug = some(get(model, dataset, aug=lambda x: x > 0, exp=0))\n",
    "    exp = some(get(model, dataset, aug=0, exp=lambda x: x > 0))\n",
    "    augs = sorted(set(a.aug for a in aug).union(set(a.aug for a in exp)))\n",
    "    sigs = sorted(set(s.sig for s in aug).union(set(a.sig for a in exp)))\n",
    "\n",
    "    out = torch.empty(len(sigs)).fill_(float('nan'))\n",
    "    for s in sigs:\n",
    "        sub_exp = [r for r in exp if r.sig == s]\n",
    "        acc = [r.acc for r in sub_exp]\n",
    "        out[sigs.index(s)] = sub_exp[acc.index(max(acc))].rob\n",
    "\n",
    "    baseline = clean.rob\n",
    "    best = max([a.rob for a in aug])\n",
    "    if plot:\n",
    "        mn = min(out[out == out].min().item(), baseline, best)\n",
    "        mx = max(out[out == out].max().item(), baseline, best)\n",
    "        ys = torch.linspace(mn, mx, 5)\n",
    "        plt.figure(figsize=figsize)\n",
    "        plt.plot(out.numpy(), '8-', label='Ours')\n",
    "        plt.axhline(baseline, linewidth=2, color='k', linestyle='--', label='Baseline')\n",
    "        plt.axhline(best, linewidth=2, color='r', linestyle='--', label='Best aug')\n",
    "        plt.ylabel('Robustness $\\Re$', fontsize=fontsize)\n",
    "        plt.xlabel('Training $\\sigma$', fontsize=fontsize)\n",
    "        plt.yticks([round(float(y), 4) for y in ys], fontsize=fontsize)\n",
    "        plt.xticks(range(len(sigs)), [str(s) for s in sigs], fontsize=fontsize)\n",
    "        plt.legend(fontsize=fontsize*.9)\n",
    "        plt.grid()\n",
    "        plt.show()\n",
    "    return baseline, best, out\n",
    "\n",
    "for model, dataset in tol.keys():\n",
    "    print(model, dataset)\n",
    "    baseline, best, out = comparable_results(model, dataset, plot=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
