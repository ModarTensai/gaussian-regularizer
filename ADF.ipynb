{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run package tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# import unittest\n",
    "# import network_moments.torch.gaussian as gnm\n",
    "# runner = unittest.TextTestRunner(sys.stdout, verbosity=2)\n",
    "# load = unittest.TestLoader().loadTestsFromModule\n",
    "# result = runner.run(unittest.TestSuite([\n",
    "#     load(gnm.relu.tests),\n",
    "#     load(gnm.affine.tests),\n",
    "#     load(gnm.net.adf.tests),\n",
    "# ]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODOs:\n",
    "\n",
    " - Fix relu covariance in case of zero-mean and zero-variance\n",
    " - Support scalar variance for affine.batch_moments()\n",
    " - Make computing AS faster for diagonal input variance\n",
    " - Backprobagation through the moments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from time import time\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import datasets\n",
    "import network_moments.torch.gaussian as gnm\n",
    "from torch.distributions import MultivariateNormal as gaussian\n",
    "\n",
    "# %matplotlib widget\n",
    "plt.style.use('dark_background')\n",
    "\n",
    "def timeit(stmt):\n",
    "    out = get_ipython().run_line_magic('timeit', '-o -q {}'.format(stmt))\n",
    "    return out.average, out.stdev\n",
    "\n",
    "def get_gaussian(dims, sigma=1, zero_mean=False, mbatch=1, vbatch=1,\n",
    "                 dtype=torch.float64, device='cpu'):\n",
    "    mean = torch.randn(mbatch, dims, dtype=dtype, device=device) * sigma\n",
    "    if zero_mean:\n",
    "        mean.zero_()\n",
    "    cov = gnm.utils.rand.definite(dims, norm=sigma ** 2,\n",
    "                                  batch=vbatch, dtype=dtype, device=device)\n",
    "    var = cov.diagonal(dim1=-2, dim2=-1)\n",
    "    return mean, cov, var\n",
    "\n",
    "\n",
    "def get_net(num_layers=4, dims=2, bias_in_first_layer=True, verbose=False):\n",
    "    net = gnm.net.Sequential(*[\n",
    "        layer for i in range(num_layers) for layer in (\n",
    "            torch.nn.Linear(dims, dims, bias=bias_in_first_layer or i > 0),\n",
    "            torch.nn.ReLU(inplace=True),\n",
    "        )\n",
    "    ][:-1]).double().eval()\n",
    "    if verbose:\n",
    "        print(net)\n",
    "\n",
    "    relu = 2 * max(num_layers - 1, 1) - 1  # index of linearization layer\n",
    "    lrs = gnm.net.Sequential.split_layers(net)\n",
    "    tsl = gnm.net.Sequential.encapsulate(\n",
    "        net[:relu], net[relu:relu + 1], net[relu + 1:])\n",
    "    if verbose:\n",
    "        print('Linearizing around layer {}.'.format(relu % len(net)))\n",
    "    return net, lrs, tsl\n",
    "\n",
    "\n",
    "def get_image(loader, index):\n",
    "    for images, _ in loader:\n",
    "        if index < len(images):\n",
    "            image = images[index:index + 1]\n",
    "            index = -1\n",
    "            break\n",
    "        index -= len(images)\n",
    "    if index >= 0:\n",
    "        print('Couldn\\'t find the image at index {} !!'.format(index))\n",
    "        image = None\n",
    "    return image\n",
    "\n",
    "\n",
    "def error(a, b):\n",
    "    if a.size() != b.size():\n",
    "        return float('inf')\n",
    "    return (a - b).abs().mean().item()\n",
    "#     return ((a / b) - 1).abs().mean().item()\n",
    "#     return ((a - b).norm() / a.norm()).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9975166666666667\n"
     ]
    }
   ],
   "source": [
    "cuda = False\n",
    "lenet = gnm.net.LeNet().to('cuda' if cuda else 'cpu')\n",
    "lenet.load_state_dict(torch.load('models/mnist/lenet.pt'))\n",
    "mnist_train = datasets.MNIST('data/mnist', train=True,\n",
    "                             transform=lenet.default_transforms())\n",
    "mnist_train_loader = torch.utils.data.DataLoader(mnist_train,\n",
    "                                                 batch_size=5000,\n",
    "                                                 pin_memory=cuda,\n",
    "                                                 num_workers=4 if cuda else 0,\n",
    "                                                 shuffle=False,\n",
    "                                                 drop_last=False)\n",
    "# print(lenet.accuracy(mnist_train_loader, 'cuda' if cuda else 'cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda = False\n",
    "alexnet = gnm.net.AlexNet().to('cuda' if cuda else 'cpu')\n",
    "alexnet.load_state_dict(torch.load('models/imagenet/alexnet.pt'))\n",
    "imagenet_valid = datasets.ImageFolder('data/imagenet/val/',\n",
    "                                      transform=alexnet.default_transforms())\n",
    "imagenet_valid_loader = torch.utils.data.DataLoader(imagenet_valid,\n",
    "                                                    batch_size=64,\n",
    "                                                    pin_memory=True,\n",
    "                                                    num_workers=4,\n",
    "                                                    shuffle=False,\n",
    "                                                    drop_last=False)\n",
    "# print(alexnet.accuracy(imagenet_valid_loader, 'cuda' if cuda else 'cpu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance comparison (forward pass vs ADF vs TSL)\n",
    "\n",
    "NOTE: ADF is 10 times slower than the forward pass and TSL is 10 times slower than ADF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dims = 100\n",
    "# mu, cov, var = get_gaussian(dims)\n",
    "# net, lrs, tsl = get_net(num_layers=7, dims=dims)\n",
    "\n",
    "# # forward pass time\n",
    "# %timeit net(mu)\n",
    "# # 189 µs ± 1.2 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n",
    "\n",
    "# # independent ADF time\n",
    "# %timeit gnm.net.adf.gaussian(lrs, mu, var, independent=True)\n",
    "# # 1.35 ms ± 9.09 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n",
    "\n",
    "# # independent TSL time\n",
    "# %timeit gnm.net.adf.gaussian(tsl, mu, var, independent=True)\n",
    "# # 14.9 ms ± 651 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
    "\n",
    "# # TSL time\n",
    "# %timeit gnm.net.adf.gaussian(tsl, mu, cov, independent=False)\n",
    "# # 36.1 ms ± 4.11 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Profile $\\mathbf{A}\\mathbf{\\Sigma}\\mathbf{A}^\\top$ if we know $\\mathbf{A}$ versus our trick\n",
    "\n",
    "We are computing the covariance of the linearization of a neural network around a certain point. One could say that this is a memory and computation tradeoff where we can just save the linearizations and compute the covariance directly.\n",
    "\n",
    "NOTE: (Cache, Trick, ASAT) is ordered from faster to slower on both CPU and GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trick_fn(net, mu, cov):\n",
    "    with torch.no_grad():\n",
    "        AS = gnm.utils.jac_at_x(net, mu, gnm.utils.sqrtm(cov))\n",
    "    return AS.t().mm(AS)\n",
    "\n",
    "def asat_fn(net, mu, cov):\n",
    "    A = gnm.utils.linearize(net, mu.view(1, -1), True)[0]\n",
    "    return A.mm(cov).mm(A.t())\n",
    "\n",
    "def cache_fn(A, cov):\n",
    "    return A.mm(cov).mm(A.t())\n",
    "\n",
    "def asat_cache_trick(device, trials=7, dim=1000, layers=10, sigma=10, dtype=torch.float64, file=None):\n",
    "    try:\n",
    "        dims, times = torch.load(file)\n",
    "    except:\n",
    "        times = []\n",
    "        dims = [2**i for i in range(13, 15)]\n",
    "        for n in gnm.utils.verbosify(dims):\n",
    "            net = gnm.net.Sequential(*[\n",
    "                layer for i in range(1, layers + 1) for layer in (\n",
    "                    torch.nn.Linear(n if i == 1 else dim,\n",
    "                                    n if i == layers else dim),\n",
    "                    torch.nn.ReLU(inplace=True),\n",
    "                )\n",
    "            ][:-1]).to(device, dtype).eval()\n",
    "\n",
    "            mu, cov, _ = get_gaussian(n, sigma=sigma, dtype=dtype, device=device)\n",
    "            mu, cov = mu[0], cov[0]\n",
    "\n",
    "            t = time()\n",
    "            for _ in range(trials):\n",
    "                r = asat_fn(net, mu, cov)\n",
    "            asat = (time() - t) / trials\n",
    "\n",
    "            t = time()\n",
    "            for _ in range(trials):\n",
    "                r = trick_fn(net, mu, cov)\n",
    "            trick = (time() - t) / trials\n",
    "            \n",
    "            A = gnm.utils.linearize(net, mu.view(1, -1), True)[0]\n",
    "            t = time()\n",
    "            for _ in range(trials):\n",
    "                r = cache_fn(A, cov)\n",
    "            cache = (time() - t) / trials\n",
    "            \n",
    "            times.append((asat, trick, cache))\n",
    "        if file is not None:\n",
    "            torch.save((dims, times), file)\n",
    "    plt.figure()\n",
    "    plt.tick_params('y', right=True)\n",
    "    plt.plot(dims, [t[0] for t in times], '+-b', label='ASA^T')\n",
    "    plt.plot(dims, [t[1] for t in times], '+-r', label='Trick')\n",
    "    plt.plot(dims, [t[2] for t in times], '+-g', label='Cache')\n",
    "    plt.ylabel('Average time in seconds')\n",
    "    plt.xlabel('Number of dimensions')\n",
    "    plt.xscale('log')\n",
    "    plt.yscale('log')\n",
    "    plt.title('Profiling on ' + str(device))\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "asat_cache_trick(device='cpu', file='data/static/asat_cpu.pt')\n",
    "asat_cache_trick(device='cuda', file='data/static/asat_gpu.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing the covariance of ReLU for general Gaussian input\n",
    "\n",
    "Which is more accurate when computing the output covariance of ReLU?\n",
    " - To copy the input covariance\n",
    " - Replace the output variance with Hinton's expressions\n",
    "\n",
    "Test all four combinations {(copy, replace), (copy), (replace), ()}\n",
    "\n",
    "NOTE: The best approach is to replace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def covariance_computation(n=100, sigma=100, count=100000):\n",
    "    mu = sigma * torch.randn(n, dtype=torch.float64)\n",
    "    cov = gnm.utils.rand.definite(n, norm=sigma ** 2, dtype=torch.float64)\n",
    "    out = gaussian(mu, cov).sample((count,)).clamp(min=0.0)\n",
    "    mc_mean = out.mean(0)\n",
    "    mc_cov = gnm.utils.cov(out)\n",
    "    mc_var = mc_cov.diag()\n",
    "    def ocov(copy=False, replace=True):\n",
    "        if copy:\n",
    "            out_cov = cov.clone()\n",
    "        else:\n",
    "            out_cov = gnm.relu.zero_mean_covariance(cov)\n",
    "        if replace:\n",
    "            out_mu, out_var = gnm.relu.moments(mu, cov.diag())\n",
    "            out_cov.diagonal(dim1=-2, dim2=-1).copy_(out_var)\n",
    "        return out_cov\n",
    "    print('copy only:', error(mc_cov, ocov(True, False)))\n",
    "    print('copy and replace:', error(mc_cov, ocov(True, True)))\n",
    "    print('neither:', error(mc_cov, ocov(False, False)))\n",
    "    print('replace:', error(mc_cov, ocov(False, True)))  # gnm.relu.batch_moments is using this method\n",
    "\n",
    "covariance_computation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ADF vs two-stage vs one-stage linearization tightness\n",
    "\n",
    "NOTE: 2-stage and 1-stage sometimes give the same error\n",
    "\n",
    "TODO: Binary classification visualization then test on MNIST and LeNet with two points (one is close to the decision boundary and the other is far)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "def adf_vs_sl(dims=2, num_layers=4, bias_in_first_layer=True, zero_mean=False,\n",
    "              samples_count=int(1e7), sigmas=(10,), verbose=False):\n",
    "    net, lrs, tsl = get_net(num_layers, dims, bias_in_first_layer, verbose)\n",
    "    mu, cov, var = get_gaussian(dims, 1, zero_mean)\n",
    "    samples = gaussian(mu[0] * 0, cov[0]).sample((samples_count,))\n",
    "\n",
    "    errors = []\n",
    "    for sigma in (gnm.utils.verbosify(sigmas) if not verbose else sigmas):\n",
    "        v = sigma ** 2\n",
    "        out = net(samples * sigma + mu[0])\n",
    "        mc_mean = out.mean(dim=0, keepdim=True)\n",
    "        mc_var = out.var(dim=0, keepdim=True)\n",
    "\n",
    "        adf_m, adf_v = gnm.net.adf.gaussian(lrs, mu, var * v, independent=True)\n",
    "        adf_m_err = error(mc_mean, adf_m)\n",
    "        adf_v_err = error(mc_var, adf_v)\n",
    "        if verbose:\n",
    "            print('ADF errors           :', [adf_m_err, adf_v_err])\n",
    "\n",
    "        lin2_m, lin2_v = gnm.net.adf.gaussian(tsl, mu, cov * v,\n",
    "                                              independent=False, linearize=True)\n",
    "        lin2_m_err = error(mc_mean, lin2_m)\n",
    "        lin2_v_err = error(mc_var, lin2_v)\n",
    "        if verbose:\n",
    "            print('2-stage linearization:', [lin2_m_err, lin2_v_err])\n",
    "\n",
    "        lin2i_m, lin2i_v = gnm.net.adf.gaussian(tsl, mu, cov * v,\n",
    "                                                independent=True, linearize=True)\n",
    "        lin2i_m_err = error(mc_mean, lin2i_m)\n",
    "        lin2i_v_err = error(mc_var, lin2i_v)\n",
    "        if verbose:\n",
    "            print('Indep. 2-stage lin   :', [lin2i_m_err, lin2i_v_err])\n",
    "\n",
    "        lin1_m, lin1_v = gnm.net.adf.gaussian(\n",
    "            [lambda x: net(x)],  # pylint: disable=W0108\n",
    "            mu, cov * v, independent=False, linearize=True\n",
    "        )\n",
    "        lin1_m_err = error(mc_mean, lin1_m)\n",
    "        lin1_v_err = error(mc_var, lin1_v)\n",
    "        if verbose:\n",
    "            print('1-stage linearization:', [lin1_m_err, lin1_v_err])\n",
    "        errors.append(((adf_m_err, adf_v_err),\n",
    "                       (lin2i_m_err, lin2i_v_err),\n",
    "                       (lin2_m_err, lin2_v_err),\n",
    "                       (lin1_m_err, lin1_v_err)))\n",
    "\n",
    "    if len(sigmas) > 1:\n",
    "        plt.figure()\n",
    "        plt.plot(sigmas, [e[0][0] for e in errors], 'b', label='ADF')\n",
    "        plt.plot(sigmas, [e[1][0] for e in errors], 'o-w', label='Li2')\n",
    "        plt.plot(sigmas, [e[2][0] for e in errors], '+-r', label='Ln2')\n",
    "        plt.plot(sigmas, [e[3][0] for e in errors], 'g', label='Ln1')\n",
    "        plt.ylabel('Error')\n",
    "        plt.xlabel('sigma')\n",
    "        plt.title('Mean Errors')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(sigmas, [e[0][1] for e in errors], 'b', label='ADF')\n",
    "        plt.plot(sigmas, [e[1][1] for e in errors], 'o-w', label='Li2')\n",
    "        plt.plot(sigmas, [e[2][1] for e in errors], '+-r', label='Ln2')\n",
    "        plt.plot(sigmas, [e[3][1] for e in errors], 'g', label='Ln1')\n",
    "        plt.ylabel('Error')\n",
    "        plt.xlabel('sigma')\n",
    "        plt.title('Variance Errors')\n",
    "#         plt.yscale('log')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "# print('Sanity checks:')  # small linearization errors\n",
    "# adf_vs_sl(num_layers=2, bias_in_first_layer=False,\n",
    "#           zero_mean=True, sigmas=(1,), verbose=True)\n",
    "\n",
    "print('\\nBehaviour of sigmas:')\n",
    "adf_vs_sl(num_layers=5, sigmas=torch.linspace(1, 10, 10).numpy().tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variance tightness with similar computation constraints\n",
    "\n",
    "To compute the output variance using our expressions, we need to do a number of forward passes. One might say that doing Monte-Carlo estimation using a number of samples equal to the number of forward passes we need for our expressions might actually be tighter than our method. So, we need to verify this with varying dimensionlaity and noise level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def constrained_variance(dim=100, layers=3, sigma=1, count=10000,\n",
    "                         dtype=torch.float64, device='cpu'):\n",
    "    for n in [2**i for i in range(1, 13)]:\n",
    "        net = gnm.net.Sequential(*[\n",
    "            layer for i in range(1, layers + 1) for layer in (\n",
    "                torch.nn.Linear(n if i == 1 else dim,\n",
    "                                n if i == layers else dim),\n",
    "                torch.nn.ReLU(inplace=True),\n",
    "            )\n",
    "        ][:-1]).to(device, dtype).eval()\n",
    "        relu = 2 * max(num_layers - 1, 1) - 1  # index of linearization layer\n",
    "        tsl = gnm.net.Sequential.encapsulate(\n",
    "            net[:relu], net[relu:relu + 1], net[relu + 1:])\n",
    "        mu, cov, _ = get_gaussian(n, sigma=sigma, dtype=dtype, device=device)\n",
    "\n",
    "        dist = gaussian(mu[0], cov[0])\n",
    "        samples = dist.sample((count,))\n",
    "        mc_var = net(samples).var(dim=0)\n",
    "\n",
    "        samples = dist.sample((n,))\n",
    "        small_mc_var = net(samples).var(dim=0)\n",
    "        exp_var = gnm.net.adf.gaussian(tsl, mu, cov, independent=False)\n",
    "\n",
    "        small_mc_error = error(mc_var, small_mc_var)\n",
    "        exp_error = error(mc_var, exp_var)\n",
    "\n",
    "        print(small_mc_error - exp_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussianity test of the output of each layer of neural networks\n",
    "\n",
    "ADF is assuming that the output of each layer in the neural network is uncorrelated Gaussian.\n",
    "The Gaussianity of data samples can be tested using either [hypothesis testing](https://link.springer.com/content/pdf/10.1007%2Fs00362-002-0119-6.pdf) (whether there is sufficient evidence that the data is Guassian or not) or by estimating the PDF as a histogram and compare it to the PDF of the best Gaussian fit. [This](https://en.wikipedia.org/wiki/Multivariate_normal_distribution#Multivariate_normality_tests) is an example of a hypothesis test for a multivariate Gaussian but first it assumes that the covariance matrix is full-rank. However, in our case, the covariance matrix is most probably rank deficient since the affine transformations are themselves rank deficient. Plus, after the ReLU some units might actually be almost determistic zero which makes this assumption even more strict. However, in this case, we can work with each element and [test its Gaussianity](https://machinelearningmastery.com/a-gentle-introduction-to-normality-tests-in-python/) independently.\n",
    "\n",
    "NOTE: Each affine after a ReLU manages, somehow, to return the distribution to Gaussian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_normal_and_display(x):\n",
    "    plt.figure()\n",
    "    fit = gnm.utils.stats.gaussian.fit(x)\n",
    "    label = 'PDF fits ~{:.2f}%'.format(100 * fit['similarity'])\n",
    "    plt.plot(fit['xs'].numpy(), fit['pdf'].numpy(), 'c', label=label)\n",
    "    label = 'N({:.2f}, {:.2f}^2)'.format(fit['mean'], fit['std'])\n",
    "    plt.plot(fit['xs'].numpy(), fit['fit'].numpy(), 'g', label=label)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "def display_layer_gaussianity(normality):\n",
    "    bins = gnm.utils.stats.num_hist_bins(normality, min_bins=50)\n",
    "    hist = normality.histc(bins, min=0, max=1)\n",
    "    xs = torch.linspace(0, 100, bins)\n",
    "    plt.figure()\n",
    "    plt.plot(xs.numpy(), hist.numpy())\n",
    "    plt.xlabel('Gaussianity fit')\n",
    "    plt.ylabel('Number of units')\n",
    "    msg = 'Mean fit: {:.2f}% ({} units : {} bins)'\n",
    "    plt.title(msg.format(100 * normality.mean(), normality.numel(), bins))\n",
    "    plt.show()\n",
    "    \n",
    "def network_gaussianity(net, mu, sigma, count):\n",
    "    if torch.is_tensor(sigma) and sigma.numel() > 1:\n",
    "        if sigma.dim() == 1:\n",
    "            sigma = sigma.diag()\n",
    "    else:\n",
    "        sigma = gnm.utils.rand.definite(mu.numel(), norm=sigma ** 2,\n",
    "                                        dtype=mu.dtype, device=mu.device)\n",
    "    print('Testing the Gaussianity of the output of each layer in {}\\n'\n",
    "          'with a Gaussian input that has a covariance matrix of norm {}\\n'\n",
    "          'using Monte-Carlo estimation with {} samples around the image\\n'.format(\n",
    "              type(net).__name__, sigma.norm(), count\n",
    "          ))\n",
    "\n",
    "    x = gaussian(mu.view(-1), sigma).sample((count,)).view(-1, *mu.shape[1:])\n",
    "    for layer in gnm.net.Sequential.split_layers(net):\n",
    "        x = layer(x)\n",
    "        if isinstance(layer, gnm.utils.Flatten):\n",
    "            continue\n",
    "        if hasattr(layer, 'layers'):\n",
    "            for el in layer.layers:\n",
    "                print(el)\n",
    "        else:\n",
    "            print(layer)\n",
    "        reshaped_x = x.view(x.size(0), -1)\n",
    "        normality = gnm.utils.stats.gaussian.gaussianity(reshaped_x, std_threshold=1e-5)\n",
    "        normality[normality > 0.99] = 0  # remove deterministic (variance = 0)\n",
    "#         fit_normal_and_display(reshaped_x[:, normality.argmin()])\n",
    "        display_layer_gaussianity(normality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# network_gaussianity(lenet, get_image(mnist_train_loader, 1) + 127.5, sigma=64, count=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# network_gaussianity(lenet, torch.randn(1, 1, 28, 28), sigma=0.3, count=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# net = gnm.net.Sequential(\n",
    "#     torch.nn.Linear(1, 2),\n",
    "#     torch.nn.ReLU(inplace=True),\n",
    "#     torch.nn.Linear(2, 4),\n",
    "#     torch.nn.ReLU(inplace=True),\n",
    "#     torch.nn.Linear(4, 15),\n",
    "#     torch.nn.ReLU(inplace=True),\n",
    "#     torch.nn.Linear(15, 50),\n",
    "#     torch.nn.ReLU(inplace=True),\n",
    "#     torch.nn.Linear(50, 500),\n",
    "# ).double().eval()\n",
    "# network_gaussianity(net, torch.randn(1, 1).double(), sigma=0.3, count=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing the output variance of affine for Gaussian input $\\mathbf{x}\\sim\\mathcal{N}\\left(\\mathbf{\\mu}, \\mathbf{\\Sigma} =\\text{diag}\\left(\\mathbf{v}\\right)\\right)$\n",
    "\n",
    "The variance of affine ($\\mathbf{A}\\mathbf{x}+\\mathbf{b}$) is $\\text{diag}\\left(\\mathbf{A}\\mathbf{\\Sigma}\\mathbf{A}^\\top\\right) = \\text{diag}\\left(\\mathbf{A}\\text{diag}\\left(\\mathbf{v}\\right)\\mathbf{A}^\\top\\right) = (\\mathbf{A}^2)\\mathbf{v}$ (because $\\mathbf{v} \\geq \\mathbf{0}$)\n",
    "\n",
    "If $\\mathbf{v} = \\sigma^2\\mathbf{1}$, The variance will be $\\sigma^2\\text{diag}\\left(\\mathbf{A}\\mathbf{A}^\\top\\right) = \\sigma^2\\left(\\mathbf{A}^2\\right)\\mathbf{1} = \\sigma^2\\ \\text{sum_columns}\\left(\\mathbf{A}^2\\right)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import network_moments.torch.gaussian as gnm\n",
    "\n",
    "class Net(gnm.net.Sequential):\n",
    "    def __init__(self, conv=True, shallow=False):\n",
    "        if conv:\n",
    "            part1 = [\n",
    "                torch.nn.Conv2d(3, 2, kernel_size=5),\n",
    "                gnm.utils.Flatten(),\n",
    "            ]\n",
    "        else:\n",
    "            part1 = [torch.nn.Linear(5, 72)]\n",
    "        part2 = [] if shallow else [\n",
    "            torch.nn.ReLU(inplace=True),\n",
    "            torch.nn.Linear(10, 2),\n",
    "            torch.nn.ReLU(inplace=True),\n",
    "            torch.nn.Linear(2, 2),\n",
    "        ]\n",
    "        super().__init__(\n",
    "            *part1,\n",
    "            torch.nn.ReLU(inplace=True),\n",
    "            torch.nn.Linear(72, 10),\n",
    "            *part2,\n",
    "        )\n",
    "        \n",
    "    def mean(self, mu, var):\n",
    "        '''Compute the output mean of the network for gaussian input.\n",
    "        \n",
    "        Args:\n",
    "            mu: Input mean (Batch, *Size).\n",
    "            var: The input variance (Size) or a scalar.\n",
    "            \n",
    "        Returns:\n",
    "            The output mean of the network.\n",
    "        '''\n",
    "        layer = self[0]\n",
    "        if not torch.is_tensor(var):\n",
    "            var = torch.tensor(var, dtype=mu.dtype, device=mu.device)\n",
    "        if isinstance(layer, torch.nn.Linear):\n",
    "            if not isinstance(self[1], torch.nn.ReLU):\n",
    "                raise ValueError('The second layer of the network must be a ReLU')\n",
    "            layers = self[2:]\n",
    "            w = layer.weight\n",
    "            affine_mu = layer(mu)\n",
    "            if var.numel() == 1:\n",
    "                affine_std = ((w * w).sum(1) * var).sqrt().unsqueeze_(0)\n",
    "            else:\n",
    "                affine_std = (w * w).mv(var).sqrt().unsqueeze_(0)\n",
    "        elif isinstance(layer, torch.nn.Conv2d):\n",
    "            if not isinstance(self[1], gnm.utils.Flatten):\n",
    "                raise ValueError('The second layer of the network must be a gnm.utils.Flatten')\n",
    "            if not isinstance(self[2], torch.nn.ReLU):\n",
    "                raise ValueError('The third layer of the network must be a ReLU')\n",
    "            layers = self[3:]\n",
    "            w = layer.weight\n",
    "            affine_mu = self[1](layer(mu))\n",
    "            if var.numel() == 1:\n",
    "                var = var.repeat(1, *mu.shape[1:])\n",
    "            else:\n",
    "                var = var.view(1, *mu.shape[1:])\n",
    "            affine_std = self[1](torch.nn.functional.conv2d(var, w**2,\n",
    "                                                            stride=layer.stride,\n",
    "                                                            padding=layer.padding,\n",
    "                                                            dilation=layer.dilation,\n",
    "                                                            groups=layer.groups).sqrt())\n",
    "        else:\n",
    "            msg = 'Don\\'t know how to compute the moments for {}'\n",
    "            raise NotImplemented(msg.format(type(layer)))\n",
    "        relu_mu = gnm.relu.mean(affine_mu, affine_std, std=True)\n",
    "        return self.forward(relu_mu, layers=layers)\n",
    "    \n",
    "    @staticmethod\n",
    "    def test_linear(shallow=True, n=7, sigma=10, count=100000, dtype=torch.float64):\n",
    "        net = Net(conv=False, shallow=shallow).to(dtype)\n",
    "        if not isinstance(net[0], torch.nn.Linear):\n",
    "            raise ValueError('The first layer of the network must be a torch.nn.Linear')\n",
    "        print(net)\n",
    "        mu = sigma * torch.randn(n, net[0].in_features, dtype=dtype)\n",
    "        var = sigma**2 * torch.rand(mu.size(-1), dtype=dtype)\n",
    "        normal_samples = torch.distributions.MultivariateNormal(\n",
    "            mu[0, ...] * 0, var.diag()).sample((count,))\n",
    "        for i in range(mu.size(0)):\n",
    "            with torch.no_grad():\n",
    "                samples = normal_samples + mu[i, ...]\n",
    "                out_mu = net.mean(mu[i:i+1, ...], var)\n",
    "                mc_mu = net(samples).mean(0, keepdim=True)\n",
    "            print(round((out_mu / mc_mu).abs().mean().item(), 1))\n",
    "    \n",
    "    @staticmethod\n",
    "    def test_conv2d(shallow=True, n=7, sigma=10, count=100000, dtype=torch.float64):\n",
    "        net = Net(conv=True, shallow=shallow).to(dtype)\n",
    "        if not isinstance(net[0], torch.nn.Conv2d):\n",
    "            raise ValueError('The first layer of the network must be a torch.nn.Conv2d')\n",
    "        print(net)\n",
    "        mu = sigma * torch.randn(n, net[0].in_channels, 10, 10, dtype=dtype)\n",
    "        var = sigma**2 * torch.rand(*mu.shape[1:], dtype=dtype)\n",
    "        normal_samples = torch.distributions.MultivariateNormal(\n",
    "            mu[0, ...].view(-1) * 0, var.view(-1).diag()).sample((count,)).view(count, *mu[0].size())\n",
    "        for i in range(mu.size(0)):\n",
    "            with torch.no_grad():\n",
    "                samples = normal_samples + mu[i, ...]\n",
    "                out_mu = net.mean(mu[i:i+1, ...], var)\n",
    "                mc_mu = net(samples).mean(0, keepdim=True)\n",
    "            print(round((out_mu / mc_mu).abs().mean().item(), 1))\n",
    "    \n",
    "    def loss(self, x, gamma=0.4):\n",
    "        mu = self.moments(x, torch.tensor(0.3, dtype=x.dtype, device=x.device))\n",
    "        return self.forward(x).sum() + gamma * mu.sum()\n",
    "\n",
    "# Net.test_linear(shallow=True)\n",
    "# Net.test_linear(shallow=False)\n",
    "# Net.test_conv2d(shallow=True)\n",
    "# Net.test_conv2d(shallow=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
